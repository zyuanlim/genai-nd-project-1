{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4936ddb0-5a5d-442b-bf77-e6d3c0f2ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers peft trl accelerate bitsandbytes datasets\n",
    "!pip install -qU flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b29c49-2c53-4ba9-bc8e-f5429e27372f",
   "metadata": {},
   "source": [
    "---\n",
    "The task is to use a Foundation Model to detect fake news (binary classification of `real` or `fake`). The datasets in question are the `mohammadjavadpirhadi/fake-news-detection-dataset-english` available from HuggingFace datasets [here](https://huggingface.co/datasets/mohammadjavadpirhadi/fake-news-detection-dataset-english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883559c1-4f8d-468a-ab66-4f5237468e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the train and test splits\n",
    "train_dataset = load_dataset(\"mohammadjavadpirhadi/fake-news-detection-dataset-english\", split='train')\n",
    "test_dataset = load_dataset(\"mohammadjavadpirhadi/fake-news-detection-dataset-english\", split='test')\n",
    "\n",
    "# Downsample the datasets due to memory and processing time constraints\n",
    "train_dataset = train_dataset.shuffle(seed=99).select(range(5000))\n",
    "test_dataset = test_dataset.shuffle(seed=99).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801ff895-b970-489f-ac01-b80eb95c8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"real\", 1: \"fake\"}\n",
    "label2id = {\"fake\": 0, \"real\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa65f600-5207-4a68-9f4c-43c5e390f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "title: Lebanon's finances can cope with PM resignation: finance minister\n",
      "label: real\n",
      "text:\n",
      "BEIRUT (Reuters) - Lebanon and its financial institutions can cope with the impact of Prime Minister Saad Hariri s surprise resignation, Finance Minister Ali Hassan Khalil said on Monday.  We are confident in the stability of the financial and monetary situation in the country. There are no very big challenges ahead of us,  Khalil said in a televised statement after a meeting on the economy chaired by President Michel Aoun.  The state is able to finance itself,  he said.   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first example in train set which is a real news\n",
    "print(f\"\"\"\n",
    "title: {train_dataset[0]['title']}\n",
    "label: {id2label[train_dataset[0]['label']]}\n",
    "text:\n",
    "{train_dataset[0]['text']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f56f3f-539b-4ae8-ae27-e73bcbe8e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "title:  Sean Hannity Is Totally Butthurt Over This Onion Picture\n",
      "label: fake\n",
      "text:\n",
      "Sean Hannity is giddy and offended that finally, just like his Great Orange Leader, there s a bloody Sean Hannity joke picture out there, and he finally gets to share in the victim outrage.All of Hannity s righteous indignation is over an Onion (yes, the satire site) article with the headline,  Hundreds Of Miniature Sean Hannitys Burst From Roger Ailes  Corpse.  The picture is what really got under Hannity s thin yet completely abrasive skin. It should several of him, like in the movie  Alien  bursting from what looks like a white shirt.Hannity, the man who bled advertisers over a false murder accusation toward Hillary Clinton, was just appalled that his 15 year old daughter would see such a horrible picture.What is wrong with the left that they think these sorts of things are funny? https://t.co/sAxON5xxmh  Sean Hannity (@seanhannity) June 1, 2017Personally, I don t think it s funny, but whatever. And if the picture really is that offensive, why is Hannity tweeting it?Here he is talking about the picture he doesn t want seen on his show:Hannity is EXTREMELY mad at @TheOnion pic.twitter.com/TLqahE4RRJ  Brendan Karet (@bad_takes) June 2, 2017There is one stark difference between this picture and the one earlier in the week by comedian Kathy Griffin. Hers depicted actual violence. The Onion picture was graphic, yes. It was also bloody, but Hannity (all of them) was depicted as alive and well.Still, Hannity didn t want to miss his ride on the victim train. He likes to play both sides: victim and victimizer. Just. Like. Trump.Now, as you may recall, a few news cycles ago, it was Hannity who was under fire for promoting a false conspiracy theory that Hillary Clinton murdered a Democratic National Committee staffer. Hannity is also just fine with Ted Nugent s actual death threats toward Hillary Clinton and Barack Obama.It looks like Hannity s career will survive, but the right, as they always do, will attempt to take down anyone who is a convenient distraction. As for The Onion, they re loving the publicity. I never would have seen the picture if it weren t for Hannity telling me about it, and I assume I m not in the minority.Featured image via Kevin Winter/Getty Images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the second example in train set which is a fake news\n",
    "print(f\"\"\"\n",
    "title: {train_dataset[1]['title']}\n",
    "label: {id2label[train_dataset[1]['label']]}\n",
    "text:\n",
    "{train_dataset[1]['text']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611a1e1-991d-467a-9d6e-a6f09378a606",
   "metadata": {},
   "source": [
    "---\n",
    "Import the required libraries, here I'm using the Supervised Fine-Tuning Trainer (`SFTTrainer`) from the [trl](https://huggingface.co/docs/trl/en/index) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c8fce7-c105-4282-94d2-7eca94ee811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcf6d1-3119-4ee5-82a1-87305294a0df",
   "metadata": {},
   "source": [
    "The LLM foundation model that I'll be using is the latest Phi-3 model family from Microsoft. Specifically the `Phi-3-mini-4k-instruct`, a 3.8B parameters and state-of-the-art model with comparable performance to many larger 7/8B models, including `Mistral-7b-v0.1` and `Llama-3-8B-Instruct`.\n",
    "\n",
    "Source: [https://huggingface.co/microsoft/Phi-3-mini-4k-instruct](Llama-3-8B-Instruct)\n",
    "\n",
    "I'll also be fine-tuning the model with QLoRA (4-bit quantization and LoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b19e76f-7ce7-4db7-bffc-cd46680298bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9ae87f9b34353ab47fa658ba05d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "# loading the model with flash-attention support\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 3072\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d56cd3-6ff2-4747-a3f0-92e94c694cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(example, tokenizer, return_response=False):\n",
    "    \"\"\"Function to apply the chat template required by phi-3 models.\n",
    "    \n",
    "    Parameters:\n",
    "    example (record): A record of dataset\n",
    "    tokenizer (tokenizer): Tokenizer\n",
    "    return_response (boolean): Whether to append the label (answer) for fine-tuning\n",
    "\n",
    "    Returns:\n",
    "    record: A modified record of dataset\n",
    "    \"\"\"\n",
    "    news = example[\"title\"] + \"\\n\" + example[\"text\"]\n",
    "    messages =[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": f\"Please classify whether the following news is fake or real.\\n{news}\\nOnly answer in one word: `fake` or `real`.\"},\n",
    "    ]\n",
    "    if return_response:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": id2label[example[\"label\"]]})\n",
    "    example[\"messages\"] = messages\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e833497-8cfd-437f-96a2-d9e29de75160",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to train\",\n",
    ")\n",
    "\n",
    "processed_test_dataset = test_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dca3cbb-d250-4978-841b-609e3642aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"batch_size\": 4,\n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f144f8-14ad-4086-972b-a42b7b174d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' real'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the inference (generation) pipeline on the first example in train set which is a real news\n",
    "# The model is able to classify it correctly\n",
    "output = pipe(processed_train_dataset[0]['messages'], **generation_args)\n",
    "\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b3288d-348d-46af-b71f-81475270ee8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' fake'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the inference (generation) pipeline on the second example in train set which is a fake news\n",
    "# The model is able to classify it correctly\n",
    "output = pipe(processed_train_dataset[1]['messages'], **generation_args)\n",
    "\n",
    "output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe3b8e-8c21-475f-a1da-a6e9047dbcd2",
   "metadata": {},
   "source": [
    "Let's run an accuracy evaluation on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac26397-157f-464f-8487-1e2ff82c88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set(pipeline):\n",
    "    preds = []\n",
    "    for output in pipeline(KeyDataset(processed_test_dataset, 'messages'), **generation_args):\n",
    "        preds.append(output[0]['generated_text'].lower().strip())\n",
    "    return preds\n",
    "\n",
    "def evaluate_test_set(preds):\n",
    "    num_correct = 0\n",
    "    for pred, test_record in zip(preds, processed_test_dataset):\n",
    "        if pred.find(id2label[test_record['label']]) != -1:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(processed_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae193d3f-998b-4199-88ad-8beec835b020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3544 > 3072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.858"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict and evaluate on whole test set\n",
    "preds = predict_test_set(pipe)\n",
    "evaluate_test_set(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcca4a3-a9b2-40e5-b911-b26728495d3c",
   "metadata": {},
   "source": [
    "The model without fine-tuning is able to classify the test set with an accuracy of 85.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60aa359-424c-4c84-adbd-cbb0d3b102e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the pipeline and free up GPU memory\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed531088-b959-4f53-bb8f-d1bb773af308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the train dataset again with the answers / labels for fine-tuning\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"return_response\": True},\n",
    "    num_proc=10,\n",
    "    desc=\"Applying chat template to train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d50e7d-84b0-4b1a-8bba-7b5968fa6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model with 2 epochs\n",
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./checkpoint_dir\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }\n",
    "\n",
    "# Use a LoRA rank of 16\n",
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)\n",
    "\n",
    "# Use supervised (instruction) fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_test_dataset,\n",
    "    max_seq_length=3072,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True\n",
    ")\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44abc7b7-21f3-4e27-b007-50dc17eb9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "618dca76-e99e-4bdb-9923-d41f0509e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model and trainer and free up GPU memory\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77e19de3-66ac-406d-96e4-8fd4b5b8b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac19654d6694891bd1c2485597c50d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload the LoRA and merge with original model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    training_config['output_dir'],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89860dfa-fb79-4f36-9d22-7114a59adcd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168782b18b0844de98e481b19ca2d97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5368ed1-48cc-4c69-b538-02e942d1f5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f284759792fb4f08a457c79b951dbf15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7343999f174ac883490c653976641f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca97e0c063c2407fb4b7e822629621df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af0388626149988d81626306f2e8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zanelim/Phi-3-mini-4k-instruct-fakenews/commit/f623352e396f1fc6d51d482d923f04c019c10af9', commit_message='Upload Phi3ForCausalLM', commit_description='', oid='f623352e396f1fc6d51d482d923f04c019c10af9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(\"zanelim/Phi-3-mini-4k-instruct-fakenews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27c0ab19-9178-49e3-8d85-bc2d9af3cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the pipeline using the fine-tuned model\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8277657e-4db4-4c8e-8613-4d2d33516739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict_test_set(pipe)\n",
    "evaluate_test_set(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a91379-9335-4592-a9fb-8b1f1f7d0509",
   "metadata": {},
   "source": [
    "The model with fine-tuning is able to classify the test set with an accuracy of 99%! A significant improvement over the non-fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5263e61-81d7-44f9-9c70-1c8bb266ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds = []\n",
    "for pred, test_record in zip(preds, test_dataset):\n",
    "        if pred.find(id2label[test_record['label']]) != -1:\n",
    "            correct_preds.append((pred, id2label[test_record['label']], test_record['title'] + '\\n' + test_record['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1929cec3-b9a2-48bb-8140-37c2c4f5c5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(correct_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae147e09-c3a5-43a6-a0cc-7e3b57687aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: real\n",
      "Actual label: real\n",
      "News: Denmark set to become next European country to ban burqas\n",
      "COPENHAGEN (Reuters) - Denmark looks set to become the next European country to restrict the burqa and the niqab, worn by some Muslim women, after most parties in the Danish parliament backed some sort of ban on facial coverings. Full and partial face veils such as burqas and niqabs divide opinion across Europe, setting advocates of religious freedom against secularists and those who argue that such garments are culturally alien or a symbol of the oppression of women. The niqab covers everything but the eyes, while the burqa also covers the eyes with a transparent veil. France, Belgium, the Netherlands, Bulgaria and the German state of Bavaria have all imposed some restrictions on the wearing of full-face veils in public places.  This is not a ban on religious clothing, this is a ban on masking,  Jacob Ellemann-Jensen, spokesman for the Liberal Party, told reporters on Friday after his party, the largest in the coalition government, decided to back a ban. This would effectively mean a ban on the niqab and the burqa, he added. Around 200 women in Denmark wear such garments, according to researchers. The three-party center-right minority government, its ally the Danish People s Party and the main opposition Social Democrats have all said they are in favor of a ban, though they are still discussing how the ban should be designed and enforced.  There will come a masking ban in Denmark. That s how it is,  Foreign Minister Anders Samuelsen said on Facebook. His party, the Liberal Alliance, had previously been one of the staunchest opponents of a ban, saying it limited people s ability to freely choose their attire, but has now aligned its stance with that of the other coalition parties, the Conservatives and the Liberals.  So if it is practically possible to have such a ban without betraying ourselves or our own values, then the Liberal Alliance will vote for it,  Samuelsen said. The Social Democrats, Denmark s biggest party, has also signaled support in principle for a ban on garments such as the burqa, which it said oppressed women.  We are ready to ban the burqa if that is what it takes ... But there are some dilemmas, not least with regards to how such a ban would be enforced,  said the Social Democrats  leader Mette Fredriksen during a debate in parliament on Thursday. Norway s government in June proposed a ban on face-covering Muslim veils in kindergartens, schools and universities.             \n",
      "\n",
      "Predicted label: fake\n",
      "Actual label: fake\n",
      "News: TARGET STORES TO REMOVE GENDER LABELS FROM KIDS DEPARTMENTS\n",
      "The LGBT Mafia and PC Police doing what they do best shaming Americans and businesses into conformity. Does anyone have the courage or fortitude to fight back, or are we just going to allow these PC thugs to strip our children of the genders God clearly assigned to each of us? Target Corp. is removing gender labels from most of its children s departments after customers complained about signs designating certain toys for girls.The kids  bedding section will no longer feature boy and girl signage, and the toy department will be without labels and pink or blue paper on the shelves, Minneapolis-based Target said on its website Friday. Gender labels will remain in the kids  clothing section because of sizing and fit differences.Retailers have been moving away from gender stereotypes, and some startups have emerged to break down the divide in kids  clothing and toys. The signage that sparked the dispute at Target was for building sets, like GoldieBlox, that are targeted at girls.  As guests have pointed out, in some departments like toys, home or entertainment, suggesting products by gender is unnecessary,  Target said.  We heard you, and we agree. Right now, our teams are working across the store to identify areas where we can phase out gender-based signage to help strike a better balance. In June, Ohio mom Abi Bechtel called out Target s gender designations in its toy aisle. She posted a photo to Twitter that showed store signs for  Girls  Building Sets  next to regular  Building Sets.  The outcry was swift, with angry shoppers calling for change. It stood out to me as a good example of the way our culture tends to view boys and men as the default, normal option and girls and women as the specialized option,  Bechtel told CNN at the time.Via: Bloomberg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first 2 correctly predicted examples from test set\n",
    "for pred, label, text in correct_preds[:2]:\n",
    "    print(f\"Predicted label: {pred}\")\n",
    "    print(f\"Actual label: {label}\")\n",
    "    print(f\"News: {text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f48e13bb-18f5-49fd-8d8c-be7c97b297a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds = []\n",
    "for pred, test_record in zip(preds, test_dataset):\n",
    "        if pred.find(id2label[test_record['label']]) == -1:\n",
    "            wrong_preds.append((pred, id2label[test_record['label']], test_record['title'] + '\\n' + test_record['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e385f84-96c4-4fe2-bdd7-15e5ae1eab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61490829-70f1-430d-8d19-8645ad3618a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: fake\n",
      "Actual label: real\n",
      "News: Trump says pope will wish he was president if Vatican is attacked by Islamic State\n",
      "(Reuters) - U.S. Republican presidential candidate Donald Trump, responding to Pope Francis’ calling him “not Christian” because of his positions on immigration, said on Thursday the pope would have wished Trump was president if Islamic State attacked Vatican.  “If and when the Vatican is attacked by the ISIS, which as everyone knows is ISIS’s ultimate trophy, I can promise you that the pope would have only wished and prayed that Donald Trump would have been president,” Trump said in a speech in South Carolina, using an acronym for the militant group.  (Reporting by Mohammad Zargham in Washington) This article was funded in part by SAP. It was independently created by the Reuters editorial staff. SAP had no editorial involvement in its creation or production.\n",
      "\n",
      "Predicted label: real\n",
      "Actual label: fake\n",
      "News: RONALD REAGAN: Remembering Our Heroes [Video]\n",
      "This speech, delivered by President Ronald Reagan on Memorial Day, epitomizes our debt of gratitude to those who died in battle serving the cause of liberty here in the United States and around the world. We will never forget you! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first 2 wrongly predicted examples from test set\n",
    "for pred, label, text in wrong_preds[:2]:\n",
    "    print(f\"Predicted label: {pred}\")\n",
    "    print(f\"Actual label: {label}\")\n",
    "    print(f\"News: {text}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
