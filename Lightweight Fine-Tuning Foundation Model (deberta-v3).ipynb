{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "The task is to use a Foundation Model to detect fake news (binary classification of `real` or `fake`). The datasets in question are the `mohammadjavadpirhadi/fake-news-detection-dataset-english` available from HuggingFace datasets [here](https://huggingface.co/datasets/mohammadjavadpirhadi/fake-news-detection-dataset-english).\n",
    "\n",
    "* PEFT technique: `QLoRA (4-bit quantization and LoRA)`\n",
    "* Model: `microsoft/deberta-v3-base`\n",
    "* Evaluation approach: `accuracy`\n",
    "* Fine-tuning dataset: `mohammadjavadpirhadi/fake-news-detection-dataset-english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb3232e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script shtab is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script markdown-it is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script trl is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers peft trl accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce43e9b51a24d88b61bfc6f2f7e7a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/786 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e4cc31ff24409994f713b40bb6bdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/57.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b8d80cc4af4631916abedfa768519b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3575faf953f7497c9e7117d3ec45c675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/35918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdd799a0f7844069499cd1f7695dfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the train and test splits\n",
    "train_dataset = load_dataset(\"mohammadjavadpirhadi/fake-news-detection-dataset-english\", split='train')\n",
    "test_dataset = load_dataset(\"mohammadjavadpirhadi/fake-news-detection-dataset-english\", split='test')\n",
    "\n",
    "# Downsample the datasets due to memory and processing time constraints\n",
    "train_dataset = train_dataset.shuffle(seed=99).select(range(5000))\n",
    "test_dataset = test_dataset.shuffle(seed=99).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6d618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"real\", 1: \"fake\"}\n",
    "label2id = {\"real\": 0, \"fake\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "title: Lebanon's finances can cope with PM resignation: finance minister\n",
      "label: real\n",
      "text:\n",
      "BEIRUT (Reuters) - Lebanon and its financial institutions can cope with the impact of Prime Minister Saad Hariri s surprise resignation, Finance Minister Ali Hassan Khalil said on Monday.  We are confident in the stability of the financial and monetary situation in the country. There are no very big challenges ahead of us,  Khalil said in a televised statement after a meeting on the economy chaired by President Michel Aoun.  The state is able to finance itself,  he said.   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "title: {train_dataset[0]['title']}\n",
    "label: {id2label[train_dataset[0]['label']]}\n",
    "text:\n",
    "{train_dataset[0]['text']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a76da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "title:  Sean Hannity Is Totally Butthurt Over This Onion Picture\n",
      "label: fake\n",
      "text:\n",
      "Sean Hannity is giddy and offended that finally, just like his Great Orange Leader, there s a bloody Sean Hannity joke picture out there, and he finally gets to share in the victim outrage.All of Hannity s righteous indignation is over an Onion (yes, the satire site) article with the headline,  Hundreds Of Miniature Sean Hannitys Burst From Roger Ailes  Corpse.  The picture is what really got under Hannity s thin yet completely abrasive skin. It should several of him, like in the movie  Alien  bursting from what looks like a white shirt.Hannity, the man who bled advertisers over a false murder accusation toward Hillary Clinton, was just appalled that his 15 year old daughter would see such a horrible picture.What is wrong with the left that they think these sorts of things are funny? https://t.co/sAxON5xxmh  Sean Hannity (@seanhannity) June 1, 2017Personally, I don t think it s funny, but whatever. And if the picture really is that offensive, why is Hannity tweeting it?Here he is talking about the picture he doesn t want seen on his show:Hannity is EXTREMELY mad at @TheOnion pic.twitter.com/TLqahE4RRJ  Brendan Karet (@bad_takes) June 2, 2017There is one stark difference between this picture and the one earlier in the week by comedian Kathy Griffin. Hers depicted actual violence. The Onion picture was graphic, yes. It was also bloody, but Hannity (all of them) was depicted as alive and well.Still, Hannity didn t want to miss his ride on the victim train. He likes to play both sides: victim and victimizer. Just. Like. Trump.Now, as you may recall, a few news cycles ago, it was Hannity who was under fire for promoting a false conspiracy theory that Hillary Clinton murdered a Democratic National Committee staffer. Hannity is also just fine with Ted Nugent s actual death threats toward Hillary Clinton and Barack Obama.It looks like Hannity s career will survive, but the right, as they always do, will attempt to take down anyone who is a convenient distraction. As for The Onion, they re loving the publicity. I never would have seen the picture if it weren t for Hannity telling me about it, and I assume I m not in the minority.Featured image via Kevin Winter/Getty Images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "title: {train_dataset[1]['title']}\n",
    "label: {id2label[train_dataset[1]['label']]}\n",
    "text:\n",
    "{train_dataset[1]['text']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74dbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'microsoft/deberta-v3-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a2a39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067e865a45f491895514b8ed7c1e14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f673332ef2b64397a9326c8089fe990e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c048615acd41d5887580331583631b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/student/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f40df019c24ffd91ca1a4caf404eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a885f0fd6d444150b3911bca8c82ac25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23e05d634e94c689e18a50835c11d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee82b13827948049a6b35c806dcd558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=2048)\n",
    "\n",
    "processed_train_dataset = train_dataset.map(\n",
    "    lambda x: {'text': x[\"title\"] + \"\\n\" + x[\"text\"]},\n",
    ")\n",
    "\n",
    "processed_test_dataset = test_dataset.map(\n",
    "    lambda x: {'text': x[\"title\"] + \"\\n\" + x[\"text\"]},\n",
    ")\n",
    "tokenized_train_dataset = train_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True),\n",
    "                                            remove_columns=[\"title\", \"text\", \"subject\", \"date\"])\n",
    "tokenized_test_dataset = test_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True),\n",
    "                                          remove_columns=[\"title\", \"text\", \"subject\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755a0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, TrainingArguments, BitsAndBytesConfig,\n",
    "    DataCollatorWithPadding, Trainer, pipeline\n",
    ")\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from peft import LoraConfig, AutoPeftModelForSequenceClassification, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8da9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332372842691401bb045dafd97497243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model_kwargs = dict(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20470285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2ForSequenceClassification(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ff03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00164a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'fake', 'score': 0.5067134499549866}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pipe(processed_train_dataset[0]['text'])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562547c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'fake', 'score': 0.5067134499549866}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9503f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_set(pipeline):\n",
    "    preds = []\n",
    "    for output in pipeline(KeyDataset(processed_test_dataset, 'text')):\n",
    "        preds.append(output['label'])\n",
    "    return preds\n",
    "\n",
    "def evaluate_test_set(preds):\n",
    "    num_correct = 0\n",
    "    for pred, test_record in zip(preds, processed_test_dataset):\n",
    "        if pred == id2label[test_record['label']]:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(processed_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2991271d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2779 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "preds = predict_test_set(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b16271d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_test_set(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5cc0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the pipeline and free up GPU memory\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3a289ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eeb5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        inference_mode=False,\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee9344e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591,362 || all params: 185,015,044 || trainable%: 0.3196\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "889105bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 44:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.052767</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.028245</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.23133019638061522, metrics={'train_runtime': 2694.0387, 'train_samples_per_second': 3.712, 'train_steps_per_second': 1.856, 'total_flos': 3422276151887376.0, 'train_loss': 0.23133019638061522, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./checkpoint_dir\",\n",
    "        overwrite_output_dir=True,\n",
    "        # Set the learning rate\n",
    "        learning_rate=1e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.02824520133435726,\n",
       " 'eval_accuracy': 1.0,\n",
       " 'eval_runtime': 43.9581,\n",
       " 'eval_samples_per_second': 11.374,\n",
       " 'eval_steps_per_second': 5.687,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9322e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model()\n",
    "model.save_pretrained(\"./checkpoint_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d54930a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9037b82268a3492886a2bca18aa45066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "266884ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1140466fdbae404a93ce54b959702ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zanelim/deberta-v3-fakenews/commit/802755f480c080373ddf7be3675a144ac83831b7', commit_message='Upload model', commit_description='', oid='802755f480c080373ddf7be3675a144ac83831b7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"zanelim/deberta-v3-fakenews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29ed8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device as GPU, MPS, or CPU according to availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93e80e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"./checkpoint_dir\"\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict_test_set(pipe)\n",
    "evaluate_test_set(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds = []\n",
    "for pred, test_record in zip(preds, processed_test_dataset):\n",
    "        if pred == id2label[test_record['label']]:\n",
    "            correct_preds.append((pred, id2label[test_record['label']], test_record['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: real\n",
      "Actual label: real\n",
      "News: Denmark set to become next European country to ban burqas\n",
      "COPENHAGEN (Reuters) - Denmark looks set to become the next European country to restrict the burqa and the niqab, worn by some Muslim women, after most parties in the Danish parliament backed some sort of ban on facial coverings. Full and partial face veils such as burqas and niqabs divide opinion across Europe, setting advocates of religious freedom against secularists and those who argue that such garments are culturally alien or a symbol of the oppression of women. The niqab covers everything but the eyes, while the burqa also covers the eyes with a transparent veil. France, Belgium, the Netherlands, Bulgaria and the German state of Bavaria have all imposed some restrictions on the wearing of full-face veils in public places.  This is not a ban on religious clothing, this is a ban on masking,  Jacob Ellemann-Jensen, spokesman for the Liberal Party, told reporters on Friday after his party, the largest in the coalition government, decided to back a ban. This would effectively mean a ban on the niqab and the burqa, he added. Around 200 women in Denmark wear such garments, according to researchers. The three-party center-right minority government, its ally the Danish People s Party and the main opposition Social Democrats have all said they are in favor of a ban, though they are still discussing how the ban should be designed and enforced.  There will come a masking ban in Denmark. That s how it is,  Foreign Minister Anders Samuelsen said on Facebook. His party, the Liberal Alliance, had previously been one of the staunchest opponents of a ban, saying it limited people s ability to freely choose their attire, but has now aligned its stance with that of the other coalition parties, the Conservatives and the Liberals.  So if it is practically possible to have such a ban without betraying ourselves or our own values, then the Liberal Alliance will vote for it,  Samuelsen said. The Social Democrats, Denmark s biggest party, has also signaled support in principle for a ban on garments such as the burqa, which it said oppressed women.  We are ready to ban the burqa if that is what it takes ... But there are some dilemmas, not least with regards to how such a ban would be enforced,  said the Social Democrats  leader Mette Fredriksen during a debate in parliament on Thursday. Norway s government in June proposed a ban on face-covering Muslim veils in kindergartens, schools and universities.             \n",
      "\n",
      "Predicted label: fake\n",
      "Actual label: fake\n",
      "News: TARGET STORES TO REMOVE GENDER LABELS FROM KIDS DEPARTMENTS\n",
      "The LGBT Mafia and PC Police doing what they do best shaming Americans and businesses into conformity. Does anyone have the courage or fortitude to fight back, or are we just going to allow these PC thugs to strip our children of the genders God clearly assigned to each of us? Target Corp. is removing gender labels from most of its children s departments after customers complained about signs designating certain toys for girls.The kids  bedding section will no longer feature boy and girl signage, and the toy department will be without labels and pink or blue paper on the shelves, Minneapolis-based Target said on its website Friday. Gender labels will remain in the kids  clothing section because of sizing and fit differences.Retailers have been moving away from gender stereotypes, and some startups have emerged to break down the divide in kids  clothing and toys. The signage that sparked the dispute at Target was for building sets, like GoldieBlox, that are targeted at girls.  As guests have pointed out, in some departments like toys, home or entertainment, suggesting products by gender is unnecessary,  Target said.  We heard you, and we agree. Right now, our teams are working across the store to identify areas where we can phase out gender-based signage to help strike a better balance. In June, Ohio mom Abi Bechtel called out Target s gender designations in its toy aisle. She posted a photo to Twitter that showed store signs for  Girls  Building Sets  next to regular  Building Sets.  The outcry was swift, with angry shoppers calling for change. It stood out to me as a good example of the way our culture tends to view boys and men as the default, normal option and girls and women as the specialized option,  Bechtel told CNN at the time.Via: Bloomberg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first 2 correctly predicted examples from test set\n",
    "for pred, label, text in correct_preds[:2]:\n",
    "    print(f\"Predicted label: {pred}\")\n",
    "    print(f\"Actual label: {label}\")\n",
    "    print(f\"News: {text}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
